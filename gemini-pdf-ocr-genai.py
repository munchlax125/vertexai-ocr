# -*- coding: utf-8 -*-
import os
import re
import json
import gspread
from google.oauth2 import service_account
import vertexai
from vertexai.generative_models import GenerativeModel
from dotenv import load_dotenv
import sys
import time
from datetime import datetime

# UTF-8 ì¸ì½”ë”© ê°•ì œ ì„¤ì •
if sys.platform.startswith('win'):
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

# --- Vertex AI ì„¤ì • ---
PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT")
LOCATION = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
CREDENTIALS_PATH = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

# --- ê¸°ì¡´ ì„¤ì • ---
SERVICE_ACCOUNT_FILE = 'pdf-ocr.json'
SPREADSHEET_NAME = 'pdf-ocr'
PDF_FOLDER_PATH = './masked-pdfs/'

# --- ì¶”ì¶œ í•„ë“œ ë° í”„ë¡¬í”„íŠ¸ ---
EXTRACTION_FIELDS = [
    "ì„±ëª…", "ìƒë…„ì›”ì¼", "ì•ˆë‚´ìœ í˜•", "ê¸°ì¥ì˜ë¬´", "ì¶”ê³„ì‹œ ì ìš©ê²½ë¹„ìœ¨",
    "ì†Œë“ì¢…ë¥˜", "ì´ì", "ë°°ë‹¹", "ê·¼ë¡œ-ë‹¨ì¼", "ê·¼ë¡œ-ë³µìˆ˜",
    "ì—°ê¸ˆ", "ê¸°íƒ€", "ì¢…êµì¸ ê¸°íƒ€ì†Œë“ìœ ë¬´", "ì¤‘ê°„ì˜ˆë‚©ì„¸ì•¡", "ì›ì²œì§•ìˆ˜ì„¸ì•¡",
    "êµ­ë¯¼ì—°ê¸ˆë³´í—˜ë£Œ", "ê°œì¸ì—°ê¸ˆì €ì¶•", "ì†Œê¸°ì—…ì†Œìƒê³µì¸ê³µì œë¶€ê¸ˆ (ë…¸ë€ìš°ì‚°ê³µì œ)",
    "í‡´ì§ì—°ê¸ˆì„¸ì•¡ê³µì œ", "ì—°ê¸ˆê³„ì¢Œì„¸ì•¡ê³µì œ", "ì‚¬ì—…ì ë“±ë¡ë²ˆí˜¸", "ìƒí˜¸", "ìˆ˜ì…ê¸ˆì•¡ êµ¬ë¶„ì½”ë“œ",
    "ì—…ì¢… ì½”ë“œ", "ì‚¬ì—… í˜•íƒœ", "ê¸°ì¥ ì˜ë¬´", "ê²½ë¹„ìœ¨",
    "ìˆ˜ì…ê¸ˆì•¡", "ì¼ë°˜", "ìê°€", "ì¼ë°˜(ê¸°ë³¸)", "ìê°€(ì´ˆê³¼)"
]

json_example = "[\n" + "  {\n" + ",\n".join([f'    "{field}": "ê°’"' for field in EXTRACTION_FIELDS]) + "\n  },\n  {\n" + ",\n".join([f'    "{field}": "ê°’2"' for field in EXTRACTION_FIELDS]) + "\n  }\n]"

GEMINI_PROMPT = f"""
## ì—­í• 
ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œ ì „ì²´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, ì—¬ëŸ¬ ë‹¤ë¥¸ ìœ„ì¹˜ì™€ í˜•ì‹ì˜ í‘œë‚˜ í…ìŠ¤íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ê³  êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ë³€í™˜í•˜ëŠ” OCR ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì‘ì—… ìˆœì„œ

### 1ë‹¨ê³„: ì „ì²´ ë¬¸ì„œì—ì„œ ë‹¨ì¼ ê°’ í•„ë“œ ìŠ¤ìº”
ë¨¼ì € ë¬¸ì„œ ì „ì²´ë¥¼ ìŠ¤ìº”í•˜ì—¬ ë‹¤ìŒ í•­ëª©ë“¤ì²˜ëŸ¼ ì£¼ë¡œ í•œ ë²ˆë§Œ ë‚˜íƒ€ë‚˜ëŠ” ê°’ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤:
- "ì„±ëª…", "ìƒë…„ì›”ì¼", "ì•ˆë‚´ìœ í˜•", "ê¸°ì¥ì˜ë¬´"
- "ì¤‘ê°„ì˜ˆë‚©ì„¸ì•¡", "ì›ì²œì§•ìˆ˜ì„¸ì•¡"
- "êµ­ë¯¼ì—°ê¸ˆë³´í—˜ë£Œ", "ê°œì¸ì—°ê¸ˆì €ì¶•", "ì†Œê¸°ì—…ì†Œìƒê³µì¸ê³µì œë¶€ê¸ˆ (ë…¸ë€ìš°ì‚°ê³µì œ)" ë“±

### 2ë‹¨ê³„: ì‚¬ì—…ì†Œë“ í‘œì˜ ëª¨ë“  í–‰ ì°¾ê¸°
'ì‚¬ì—…ì¥ë³„ ìˆ˜ì…ê¸ˆì•¡' ë˜ëŠ” ìœ ì‚¬í•œ í‘œì—ì„œ **ëª¨ë“  í–‰(ë°ì´í„°)ì„ ì°¾ì•„ì£¼ì„¸ìš”**. 
- ê° í–‰ì€ í•˜ë‚˜ì˜ ì‚¬ì—…ì†Œë“ í•­ëª©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤
- **ë¹ˆ í–‰ì´ë‚˜ ëˆ„ë½ëœ í–‰ì´ ì—†ë„ë¡ ì£¼ì˜ê¹Šê²Œ í™•ì¸í•´ì£¼ì„¸ìš”**
- ë‹¤ìŒ í•„ë“œë“¤ì„ ê° í–‰ì—ì„œ ì¶”ì¶œ: "ì‚¬ì—…ì ë“±ë¡ë²ˆí˜¸", "ìƒí˜¸", "ìˆ˜ì…ì¢…ë¥˜ êµ¬ë¶„ì½”ë“œ", "ì—…ì¢… ì½”ë“œ", "ìˆ˜ì…ê¸ˆì•¡", "ê²½ë¹„ìœ¨" ë“±

### 3ë‹¨ê³„: ê° í–‰ë³„ JSON ê°ì²´ ìƒì„±
**ì‚¬ì—…ì†Œë“ í‘œì˜ ê° í–‰ë§ˆë‹¤** ë³„ë„ì˜ JSON ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:
1. í•´ë‹¹ í–‰ì˜ ì‚¬ì—… ê´€ë ¨ ë°ì´í„°ë¡œ ê°ì²´ë¥¼ ì±„ì›ë‹ˆë‹¤
2. **1ë‹¨ê³„ì—ì„œ ì°¾ì€ ëª¨ë“  ê³µí†µ ë°ì´í„°(ì„±ëª…, ìƒë…„ì›”ì¼ ë“±)ë¥¼ ë™ì¼í•˜ê²Œ ë³µì‚¬í•©ë‹ˆë‹¤**

### 4ë‹¨ê³„: ì™„ì „í•œ JSON ë°°ì—´ ìƒì„±
- **ëª¨ë“  ì‚¬ì—…ì†Œë“ í–‰ì´ í¬í•¨ë˜ë„ë¡ í™•ì¸**
- ê° ê°ì²´ëŠ” ëª¨ë“  í•„ë“œë¥¼ í¬í•¨í•´ì•¼ í•¨
- ê°’ì´ ì—†ëŠ” í•„ë“œëŠ” "N/A" ë˜ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •

## ì¤‘ìš” ì§€ì¹¨
- **"ì„±ëª…","ìƒë…„ì›”ì¼","ì‚¬ì—…ì ë“±ë¡ë²ˆí˜¸","ìƒí˜¸"ëŠ” ê°œì¸ì •ë³´ ë³´í˜¸ ë•Œë¬¸ì— ì¼ë¶€ëŸ¬ ë§ˆìŠ¤í‚¹ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ê°’ì´ ì—†ìŠµë‹ˆë‹¤. ê·¸ëƒ¥ ë¹ˆì¹¸ìœ¼ë¡œ ë‘ì„¸ìš”.
- **ì ˆëŒ€ë¡œ ë°ì´í„°ë¥¼ ëˆ„ë½í•˜ì§€ ë§ˆì„¸ìš”**
- **ëª¨ë“  ì‚¬ì—…ì†Œë“ í–‰ì„ ì°¾ì•„ ê°ê° ë³„ë„ì˜ JSON ê°ì²´ë¡œ ë§Œë“œì„¸ìš”**
- í•˜ë‚˜ì˜ ë¬¸ì„œì— ì—¬ëŸ¬ ì‚¬ì—…ì†Œë“ì´ ìˆë‹¤ë©´, ê·¸ ìˆ˜ë§Œí¼ JSON ê°ì²´ê°€ ìƒì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤

### ì¶”ì¶œí•  í•­ëª©
{', '.join(EXTRACTION_FIELDS)}

### ì¶œë ¥ í˜•ì‹ (ì—¬ëŸ¬ í–‰ì´ ìˆì„ ê²½ìš°ì˜ ì˜ˆì‹œ)
{json_example}

**ë°˜ë“œì‹œ JSON ë°°ì—´ í˜•íƒœë¡œë§Œ ì‘ë‹µí•˜ê³ , ë‹¤ë¥¸ ì„¤ëª…ì€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.**
"""

# --- ìˆ«ì ì •ì œ ëŒ€ìƒ í•„ë“œ ---
currency_fields = [
    "ì¤‘ê°„ì˜ˆë‚©ì„¸ì•¡", "ì›ì²œì§•ìˆ˜ì„¸ì•¡", "êµ­ë¯¼ì—°ê¸ˆë³´í—˜ë£Œ", "ê°œì¸ì—°ê¸ˆì €ì¶•",
    "ì†Œê¸°ì—…ì†Œìƒê³µì¸ê³µì œë¶€ê¸ˆ (ë…¸ë€ìš°ì‚°ê³µì œ)", "í‡´ì§ì—°ê¸ˆì„¸ì•¡ê³µì œ", "ì—°ê¸ˆê³„ì¢Œì„¸ì•¡ê³µì œ", "ìˆ˜ì…ê¸ˆì•¡"
]

# --- ë¡œê·¸ ì¶œë ¥ í•¨ìˆ˜ (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ìš©) ---
def log_progress(message, flush=True):
    """ì§„í–‰ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    formatted_msg = f"[{timestamp}] {message}"
    print(formatted_msg)
    if flush:
        sys.stdout.flush()

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---
def clean_currency(value: str) -> str:
    if not isinstance(value, str): return "0"
    if value.strip() in ["", "ì—†ìŒ", "N/A"]: return "0"
    cleaned = re.sub(r"[^\d]", "", value)
    return cleaned if cleaned else "0"

def safe_extract_json(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ JSON ë°°ì—´ì„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    # ì—¬ëŸ¬ íŒ¨í„´ìœ¼ë¡œ JSON ì°¾ê¸° ì‹œë„
    patterns = [
        r'\[[\s\S]*?\]',  # JSON ë°°ì—´ (ê°€ì¥ ìš°ì„ )
        r'```json\s*([\s\S]*?)\s*```',  # ë§ˆí¬ë‹¤ìš´ JSON ë¸”ë¡
        r'```\s*([\s\S]*?)\s*```',  # ì¼ë°˜ ë§ˆí¬ë‹¤ìš´ ë¸”ë¡
        r'\{[\s\S]*?\}',  # JSON ê°ì²´ (ë‹¨ì¼)
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text, re.DOTALL)
        for match in matches:
            try:
                # ë§ˆí¬ë‹¤ìš´ íŒ¨í„´ì˜ ê²½ìš°
                if '```' in pattern and isinstance(match, str):
                    json_data = json.loads(match.strip())
                else:
                    json_data = json.loads(match)
                
                # ë°°ì—´ì´ ì•„ë‹Œ ê²½ìš° ë°°ì—´ë¡œ ë³€í™˜
                if isinstance(json_data, dict):
                    return [json_data]
                elif isinstance(json_data, list):
                    return json_data
                    
            except json.JSONDecodeError:
                continue
    
    return None

def extract_data_with_vertex_ai(file_path: str, prompt: str, file_number: int, total_files: int):
    """
    Vertex AIë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ PDFì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    log_progress(f"ğŸ”„ [{file_number}/{total_files}] '{os.path.basename(file_path)}' Vertex AI OCR ë¶„ì„ ì‹œì‘...")
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"âŒ ì˜¤ë¥˜: PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ: {file_path}")

    max_retries = 3
    
    for attempt in range(max_retries):
        try:
            if max_retries > 1:
                log_progress(f"ğŸ”„ [{file_number}/{total_files}] Vertex AI OCR ì‹œë„ {attempt + 1}/{max_retries}")
            
            # Vertex AI GenerativeModel ìƒì„±
            model = GenerativeModel("gemini-2.5-flash")
            
            # íŒŒì¼ ì½ê¸°
            log_progress(f"ğŸ“¤ [{file_number}/{total_files}] '{os.path.basename(file_path)}' íŒŒì¼ ì½ëŠ” ì¤‘...")
            
            with open(file_path, 'rb') as f:
                file_data = f.read()
            
            # PDFë¥¼ base64ë¡œ ì¸ì½”ë”©
            import base64
            pdf_data = base64.b64encode(file_data).decode('utf-8')
            
            # Part ìƒì„±
            from vertexai.generative_models import Part
            pdf_part = Part.from_data(
                data=file_data,
                mime_type="application/pdf"
            )
            
            # ì½˜í…ì¸  ìƒì„±
            log_progress(f"ğŸ§  [{file_number}/{total_files}] '{os.path.basename(file_path)}' Vertex AI ë¶„ì„ ì¤‘...")
            
            response = model.generate_content([pdf_part, prompt])
            
            log_progress(f"ğŸ“„ [{file_number}/{total_files}] '{os.path.basename(file_path)}' ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (ê¸¸ì´: {len(response.text)} ë¬¸ì)")
            
            # JSON ì¶”ì¶œ
            extracted_data = safe_extract_json(response.text)
            
            if extracted_data is None:
                log_progress(f"âš ï¸ [{file_number}/{total_files}] '{os.path.basename(file_path)}' JSON ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1})")
                if attempt < max_retries - 1:
                    log_progress(f"ğŸ”„ [{file_number}/{total_files}] '{os.path.basename(file_path)}' ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    time.sleep(5)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                    continue
                else:
                    raise ValueError(f"âŒ '{os.path.basename(file_path)}' ëª¨ë“  ì‹œë„ì—ì„œ JSON ì¶”ì¶œ ì‹¤íŒ¨")
            
            log_progress(f"âœ… [{file_number}/{total_files}] '{os.path.basename(file_path)}' Vertex AI OCR ì„±ê³µ! {len(extracted_data)}ê°œ í•­ëª© ë°œê²¬")
            return extracted_data
            
        except Exception as e:
            log_progress(f"âŒ [{file_number}/{total_files}] '{os.path.basename(file_path)}' Vertex AI OCR ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(5)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°

def validate_and_fix_data(data_list, file_number, total_files, filename):
    """
    ì¶”ì¶œëœ ë°ì´í„°ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•˜ê³  ìˆ˜ì •
    """
    if not isinstance(data_list, list):
        log_progress(f"âš ï¸ [{file_number}/{total_files}] '{filename}' ë°ì´í„°ê°€ ë°°ì—´ì´ ì•„ë‹™ë‹ˆë‹¤. ë°°ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
        return [data_list] if isinstance(data_list, dict) else []
    
    validated_data = []
    for i, item in enumerate(data_list):
        if not isinstance(item, dict):
            log_progress(f"âš ï¸ [{file_number}/{total_files}] '{filename}' í•­ëª© {i+1}ì´ ê°ì²´ê°€ ì•„ë‹™ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        
        # ëª¨ë“  í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ì¶”ê°€
        for field in EXTRACTION_FIELDS:
            if field not in item:
                item[field] = "N/A"
        
        validated_data.append(item)
    
    log_progress(f"âœ… [{file_number}/{total_files}] '{filename}' ë°ì´í„° ê²€ì¦ ì™„ë£Œ. {len(validated_data)}ê°œ í•­ëª© ìœ íš¨")
    return validated_data

def add_to_spreadsheet_batch(worksheet, rows_to_append, file_number, total_files, filename):
    """ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì— ë°°ì¹˜ë¡œ ë°ì´í„° ì¶”ê°€"""
    try:
        log_progress(f"ğŸ“Š [{file_number}/{total_files}] '{filename}' êµ¬ê¸€ì‹œíŠ¸ì— {len(rows_to_append)}ê°œ í–‰ ì—…ë¡œë“œ ì¤‘...")
        worksheet.append_rows(rows_to_append)
        log_progress(f"âœ… [{file_number}/{total_files}] '{filename}' êµ¬ê¸€ì‹œíŠ¸ ì—…ë¡œë“œ ì™„ë£Œ!")
        return True
    except Exception as e:
        log_progress(f"âŒ [{file_number}/{total_files}] '{filename}' êµ¬ê¸€ì‹œíŠ¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

# --- ğŸš€ Main ---
def main():
    start_time = time.time()
    log_progress("=" * 70)
    log_progress("ğŸš€ PDF ì¼ê´„ Vertex AI OCR ì²˜ë¦¬ ë° êµ¬ê¸€ì‹œíŠ¸ ì—…ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤")
    log_progress("=" * 70)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    log_progress(f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    log_progress(f"ğŸ“‚ ì‘ì—… í´ë”: {PDF_FOLDER_PATH}")
    log_progress(f"ğŸ“Š ëŒ€ìƒ ìŠ¤í”„ë ˆë“œì‹œíŠ¸: {SPREADSHEET_NAME}")
    log_progress(f"ğŸ­ Vertex AI í”„ë¡œì íŠ¸: {PROJECT_ID}")
    log_progress(f"ğŸŒ Vertex AI ìœ„ì¹˜: {LOCATION}")
    log_progress("-" * 70)

    # --- Vertex AI ë° Google Sheets ì¸ì¦ ---
    try:
        # Vertex AI ì´ˆê¸°í™”
        if not PROJECT_ID:
            raise ValueError("GOOGLE_CLOUD_PROJECT í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        log_progress("ğŸ”‘ Vertex AI ì´ˆê¸°í™” ì¤‘...")
        
        # ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦ ì„¤ì •
        if CREDENTIALS_PATH and os.path.exists(CREDENTIALS_PATH):
            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = CREDENTIALS_PATH
            log_progress(f"ğŸ” ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦ íŒŒì¼ ì„¤ì •: {CREDENTIALS_PATH}")
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        log_progress(f"ğŸ” í™˜ê²½ ë³€ìˆ˜ í™•ì¸:")
        log_progress(f"   - GOOGLE_CLOUD_PROJECT: {PROJECT_ID}")
        log_progress(f"   - GOOGLE_CLOUD_LOCATION: {LOCATION}")
        log_progress(f"   - GOOGLE_APPLICATION_CREDENTIALS: {CREDENTIALS_PATH}")
        
        # Vertex AIìš© google-generativeai ì„¤ì •
        import vertexai
        vertexai.init(project=PROJECT_ID, location=LOCATION)
        
        log_progress("âœ… Vertex AI ì´ˆê¸°í™” ì„±ê³µ!")

        # Google Sheets ì¸ì¦
        log_progress("ğŸ“‹ Google Sheets ì—°ê²° ì¤‘...")
        scopes = [
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive"
        ]
        creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=scopes)
        client = gspread.authorize(creds)
        spreadsheet = client.open(SPREADSHEET_NAME)
        worksheet = spreadsheet.sheet1
        
        # ì˜¤ë¥˜ ë¡œê·¸ ì‹œíŠ¸ ì„¤ì •
        try:
            log_worksheet = spreadsheet.worksheet("ì˜¤ë¥˜_ë¡œê·¸")
        except gspread.exceptions.WorksheetNotFound:
            log_worksheet = spreadsheet.add_worksheet(title="ì˜¤ë¥˜_ë¡œê·¸", rows="100", cols="10")
            log_worksheet.append_row(["íŒŒì¼ ì´ë¦„", "ì˜¤ë¥˜ ë‚´ìš©", "ì²˜ë¦¬ ì‹œê°„"])
        
        log_progress("âœ… êµ¬ê¸€ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—°ê²° ì„±ê³µ!")
    except Exception as e:
        log_progress(f"âŒ ì¸ì¦ ì‹¤íŒ¨: {e}")
        return

    # í—¤ë” ì„¤ì •
    try:
        log_progress("ğŸ“ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ í—¤ë” í™•ì¸ ì¤‘...")
        first_row = worksheet.row_values(1)
        if not first_row:
            log_progress("ğŸ“ 1í–‰ì´ ë¹„ì–´ìˆì–´ í—¤ë”ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤...")
            headers = ["íŒŒì¼ì´ë¦„", "í–‰ë²ˆí˜¸"] + EXTRACTION_FIELDS
            worksheet.append_row(headers)
            log_progress("âœ… í—¤ë” ì¶”ê°€ ì™„ë£Œ!")
        else:
            log_progress("âœ… í—¤ë”ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
    except Exception as e:
        log_progress(f"âŒ í—¤ë” í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    try:
        log_progress("ğŸ“‚ PDF íŒŒì¼ ëª©ë¡ ìŠ¤ìº” ì¤‘...")
        pdf_files = [f for f in os.listdir(PDF_FOLDER_PATH) if f.lower().endswith('.pdf')]
        if not pdf_files:
            log_progress(f"âŒ '{PDF_FOLDER_PATH}' í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # íŒŒì¼ëª…ì„ ìˆ«ì ìˆœì„œë¡œ ì •ë ¬ (1.pdf, 2.pdf, 3.pdf...)
        def natural_sort_key(filename):
            try:
                # íŒŒì¼ëª…ì—ì„œ .pdf ì œê±°í•˜ê³  ìˆ«ìë¡œ ë³€í™˜
                number = int(filename.replace('.pdf', ''))
                return number
            except ValueError:
                # ìˆ«ìê°€ ì•„ë‹Œ íŒŒì¼ëª…ì€ ë§¨ ë’¤ë¡œ
                return 999999
        
        pdf_files.sort(key=natural_sort_key)
        
        log_progress(f"ğŸ“‚ ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ Vertex AIë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤")
        log_progress(f"ğŸ“‹ íŒŒì¼ ëª©ë¡: {pdf_files[:10]}{'...' if len(pdf_files) > 10 else ''}")
    except FileNotFoundError:
        log_progress(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: '{PDF_FOLDER_PATH}'")
        return

    total_rows_added = 0
    error_count = 0
    successful_files = 0

    # íŒŒì¼ ì²˜ë¦¬ ì‹œì‘
    log_progress(f"{'='*25} ğŸ“„ Vertex AI íŒŒì¼ë³„ OCR ì²˜ë¦¬ ì‹œì‘ {'='*25}")
    
    # ê° PDF íŒŒì¼ ì²˜ë¦¬
    for i, pdf_file in enumerate(pdf_files, 1):
        file_start_time = time.time()
        
        log_progress(f"")
        log_progress(f"ğŸ“„ [{i}/{len(pdf_files)}] ===== {pdf_file} Vertex AI ì²˜ë¦¬ ì‹œì‘ =====")
        log_progress("-" * 50)
        
        try:
            full_path = os.path.join(PDF_FOLDER_PATH, pdf_file)
            
            # íŒŒì¼ í¬ê¸° ì •ë³´ ì¶”ê°€
            file_size = os.path.getsize(full_path) / 1024 / 1024  # MB
            log_progress(f"ğŸ“ [{i}/{len(pdf_files)}] '{pdf_file}' íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")
            
            # Vertex AIë¡œ ë°ì´í„° ì¶”ì¶œ
            extracted_data_list = extract_data_with_vertex_ai(full_path, GEMINI_PROMPT, i, len(pdf_files))
            
            # ë°ì´í„° ê²€ì¦ ë° ìˆ˜ì •
            validated_data = validate_and_fix_data(extracted_data_list, i, len(pdf_files), pdf_file)
            
            if not validated_data:
                log_progress(f"âš ï¸ [{i}/{len(pdf_files)}] '{pdf_file}'ì—ì„œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                log_worksheet.append_row([pdf_file, "ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ", datetime.now().strftime("%Y-%m-%d %H:%M:%S")])
                continue
            
            # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì— ì¶”ê°€í•  í–‰ë“¤ ì¤€ë¹„
            log_progress(f"ğŸ“Š [{i}/{len(pdf_files)}] '{pdf_file}' ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
            rows_to_append = []
            for j, extracted_data in enumerate(validated_data):
                # ëª¨ë“  í–‰ì— íŒŒì¼ ì´ë¦„ í‘œì‹œ (í™•ì¥ì ì œê±°)
                file_name_without_ext = pdf_file.replace('.pdf', '')  # .pdf í™•ì¥ì ì œê±°
                row_number = j + 1
                
                data_row = [file_name_without_ext, row_number]
                for field in EXTRACTION_FIELDS:
                    value = extracted_data.get(field, 'N/A')
                    if isinstance(value, str):
                        value = value.replace('\n', ' ').replace('\r', ' ')
                    if field in currency_fields:
                        value = clean_currency(str(value))
                    data_row.append(str(value))
                
                rows_to_append.append(data_row)
            
            # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì— ì‹¤ì‹œê°„ ì¶”ê°€
            if rows_to_append:
                success = add_to_spreadsheet_batch(worksheet, rows_to_append, i, len(pdf_files), pdf_file)
                if success:
                    total_rows_added += len(rows_to_append)
                    successful_files += 1
                else:
                    log_worksheet.append_row([pdf_file, "ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì¶”ê°€ ì‹¤íŒ¨", datetime.now().strftime("%Y-%m-%d %H:%M:%S")])
                    error_count += 1
                    continue
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            file_end_time = time.time()
            processing_time = file_end_time - file_start_time
            
            log_progress(f"âœ… [{i}/{len(pdf_files)}] '{pdf_file}' Vertex AI ì²˜ë¦¬ ì™„ë£Œ!")
            log_progress(f"   ğŸ“Š OCR ì¶”ì¶œ: {len(validated_data)}ê°œ í•­ëª©")
            log_progress(f"   ğŸ“ ì‹œíŠ¸ ì—…ë¡œë“œ: {len(rows_to_append)}ê°œ í–‰")
            log_progress(f"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            log_progress(f"   ğŸ“ˆ ì „ì²´ ì§„í–‰ë¥ : {i}/{len(pdf_files)} ({(i/len(pdf_files)*100):.1f}%)")
            log_progress(f"===== {pdf_file} Vertex AI ì²˜ë¦¬ ì™„ë£Œ =====")

        except Exception as e:
            error_message = f"ğŸš¨ [{i}/{len(pdf_files)}] '{pdf_file}' Vertex AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            log_progress(error_message)
            
            # ì˜¤ë¥˜ ë¡œê·¸ì— ê¸°ë¡
            log_worksheet.append_row([pdf_file, str(e), datetime.now().strftime("%Y-%m-%d %H:%M:%S")])
            error_count += 1
            continue

    # ì´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
    end_time = time.time()
    total_processing_time = end_time - start_time

    # ìµœì¢… ê²°ê³¼
    log_progress(f"")
    log_progress(f"{'='*25} âœ¨ Vertex AI ì²˜ë¦¬ ì™„ë£Œ {'='*25}")
    log_progress(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_processing_time:.2f}ì´ˆ ({total_processing_time/60:.1f}ë¶„)")
    log_progress(f"ğŸ“Š ì´ ì²˜ë¦¬ëœ íŒŒì¼: {len(pdf_files)}ê°œ")
    log_progress(f"âœ… ì„±ê³µ: {successful_files}ê°œ")
    log_progress(f"âŒ ì˜¤ë¥˜: {error_count}ê°œ")
    log_progress(f"ğŸ“ ì´ ì—…ë¡œë“œ í–‰ ìˆ˜: {total_rows_added}ê°œ")
    log_progress(f"âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: {total_processing_time/successful_files:.2f}ì´ˆ/íŒŒì¼" if successful_files > 0 else "")
    
    if error_count > 0:
        log_progress(f"âš ï¸ ì˜¤ë¥˜ ìƒì„¸ ë‚´ìš©ì€ 'ì˜¤ë¥˜_ë¡œê·¸' ì‹œíŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    log_progress("ğŸ‰ ëª¨ë“  Vertex AI OCR ë° êµ¬ê¸€ì‹œíŠ¸ ì—…ë¡œë“œ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    log_progress("=" * 70)

if __name__ == '__main__':
    main()